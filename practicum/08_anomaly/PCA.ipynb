{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Faces dataset decompositions\n",
      "============================\n",
      "\n",
      "This example applies to :ref:`olivetti_faces` different unsupervised\n",
      "matrix decomposition (dimension reduction) methods from the module\n",
      ":py:mod:`sklearn.decomposition` (see the documentation chapter\n",
      ":ref:`decompositions`) .\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ecf0bd4a9d35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Load faces data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_olivetti_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/src/jma/MLWeek/venv/local/lib/python2.7/site-packages/sklearn/datasets/olivetti_faces.pyc\u001b[0m in \u001b[0;36mfetch_olivetti_faces\u001b[0;34m(data_home, shuffle, random_state, download_if_missing)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mmfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;31m# We want floating point data, but float32 is enough (there is only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# one byte of precision in the original uint8s anyway)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/src/jma/MLWeek/venv/local/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    571\u001b[0m                     \u001b[0;31m# try to load a pickle file generated with an version of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                     \u001b[0;31m# Joblib so we load it with joblib compatibility function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeff/src/jma/MLWeek/venv/local/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle_compat.pyc\u001b[0m in \u001b[0;36mload_compatibility\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipNumpyUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# More user-friendly error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_proto\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mproto\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unsupported pickle protocol: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPROTO\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported pickle protocol: 4"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================\n",
    "Faces dataset decompositions\n",
    "============================\n",
    "\n",
    "This example applies to :ref:`olivetti_faces` different unsupervised\n",
    "matrix decomposition (dimension reduction) methods from the module\n",
    ":py:mod:`sklearn.decomposition` (see the documentation chapter\n",
    ":ref:`decompositions`) .\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "# Authors: Vlad Niculae, Alexandre Gramfort\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import logging\n",
    "from time import time\n",
    "\n",
    "from numpy.random import RandomState\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import decomposition\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "###############################################################################\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "###############################################################################\n",
    "# List of the different estimators, whether to center and transpose the\n",
    "# problem, and whether the transformer uses the clustering API.\n",
    "estimators = [\n",
    "    ('Eigenfaces - RandomizedPCA',\n",
    "     decomposition.RandomizedPCA(n_components=n_components, whiten=True),\n",
    "     True),\n",
    "]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Plot a sample of the input data\n",
    "\n",
    "plot_gallery(\"First centered Olivetti faces\", faces_centered[:n_components])\n",
    "\n",
    "###############################################################################\n",
    "# Do the estimation and plot it\n",
    "\n",
    "for name, estimator, center in estimators:\n",
    "    print(\"Extracting the top %d %s...\" % (n_components, name))\n",
    "    t0 = time()\n",
    "    data = faces\n",
    "    if center:\n",
    "        data = faces_centered\n",
    "    estimator.fit(data)\n",
    "    train_time = (time() - t0)\n",
    "    print(\"done in %0.3fs\" % train_time)\n",
    "    if hasattr(estimator, 'cluster_centers_'):\n",
    "        components_ = estimator.cluster_centers_\n",
    "    else:\n",
    "        components_ = estimator.components_\n",
    "    if hasattr(estimator, 'noise_variance_'):\n",
    "        plot_gallery(\"Pixelwise variance\",\n",
    "                     estimator.noise_variance_.reshape(1, -1), n_col=1,\n",
    "                     n_row=1)\n",
    "    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n",
    "                 components_[:n_components])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================================================\n",
    "Faces recognition example using eigenfaces and SVMs\n",
    "===================================================\n",
    "\n",
    "The dataset used in this example is a preprocessed excerpt of the\n",
    "\"Labeled Faces in the Wild\", aka LFW_:\n",
    "\n",
    "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
    "\n",
    ".. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "Expected results for the top 5 most represented people in the dataset::\n",
    "\n",
    "                     precision    recall  f1-score   support\n",
    "\n",
    "  Gerhard_Schroeder       0.91      0.75      0.82        28\n",
    "    Donald_Rumsfeld       0.84      0.82      0.83        33\n",
    "         Tony_Blair       0.65      0.82      0.73        34\n",
    "       Colin_Powell       0.78      0.88      0.83        58\n",
    "      George_W_Bush       0.93      0.86      0.90       129\n",
    "\n",
    "        avg / total       0.86      0.84      0.85       282\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Train a SVM classification model\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
