{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "** Ne pas faire un \"execute all\" : la dernière cellule est _très_ lourde. **\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "# Introduction à TensorFlow\n",
    "\n",
    "Ce code est basé sur des tutoriel à [tensorflow.org](https://www.tensorflow.org/).\n",
    "\n",
    "Nous allons utiliser _softmax regression  pour MNIST.\n",
    "\n",
    "La première chose à faire est de récupérer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice : explorer les données (mnist).  Qu'est-ce que vous trouvez, comprenez ?\n",
    "\n",
    "## Softmax regression\n",
    "\n",
    "Softmax est simplement une généralisation de la fonction logit (sigmoid) qui garantie qu'un vecteur finit entre 0 et 1.  Dit différemment, l'indice qu'une entrée $x$ est dans class $i$ est\n",
    "\n",
    "$$\\mbox{evidence}_i = \\sum_j W_{i,j} x_j + b_i$$\n",
    "\n",
    "avec\n",
    "\n",
    "$$\\mbox{softmax}(x) = \\mbox{normalize}(e^x) \\iff \\mbox{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "Souvent on écrit simplement\n",
    "\n",
    "$$y = \\mbox{softmax}(Wx+b)$$\n",
    "\n",
    "## Cross-entropy\n",
    "\n",
    "$$H_{y'}(y) = -\\sum_i y_i' \\log(y_i)$$\n",
    "\n",
    "## TensorFlow (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Ici None veut dire \"de n'importe quelle longueur\".\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Nous allons apprendre W et b, donc on s'inquiète pas\n",
    "# de leur valeurs maintenant.\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Notre modèle.\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Cross-entropy, cf. théorie de l'information.\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), \n",
    "                                              reduction_indices=[1]))\n",
    "# En fait, ça manque de stabilité numérique, dans la vrai vie pensez\n",
    "# à utiliser tf.nn.(sparse_)softmax_cross_entropy_with_logits().\n",
    "\n",
    "# Optimisation, au choix.\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Initialiser toutes les variables.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Jusqu'ici nous n'avons rien fait.  Il ne s'agit que déclarations.\n",
    "# Maintenant on déclenche le calcul.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.argmax nous donne la valeur la plus importante sur une axe\n",
    "# d'un tenseur.  Donc tf.argmax(y, 1) est la labelle que notre\n",
    "# modèle trouve le plus probable pour chaque entrèe, et\n",
    "# tf.argmax(y_, 1) est la labelle correcte.\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, \n",
    "                                    y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow (II)\n",
    "\n",
    "La même chose mais une session interactive, ce qui nous permet d'être un peu plus lâche dans l'ordre d'opérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess2 = tf.InteractiveSession()\n",
    "\n",
    "x2 = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_2 = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "W2 = tf.Variable(tf.zeros([784,10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "sess2.run(tf.initialize_all_variables())\n",
    "\n",
    "y2 = tf.matmul(x2, W2) + b2\n",
    "cross_entropy2 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(y2, y_2))\n",
    "train_step2 = tf.train.GradientDescentOptimizer(0.5).minimize(\n",
    "    cross_entropy2)\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step2.run(feed_dict={x2: batch[0], y_2: batch[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction2 = tf.equal(tf.argmax(y2, 1), tf.argmax(y_2, 1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))\n",
    "print(accuracy2.eval(feed_dict={x2: mnist.test.images, \n",
    "                                y_2: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow (III)\n",
    "\n",
    "Construisons un modèle plus soutenu : un multilayer convolutional network.\n",
    "\n",
    "Nous avons vu des neurones de type rectified linear, $f(x) = max(0, x)$.  Un tel neurone s'appelle un ReLU.\n",
    "\n",
    "On peut le rendre différentiable ainsi : $ f(x) = \\ln\\left(1+e^x\\right)$.\n",
    "\n",
    "Remarques :\n",
    "* On leur donne un biais légèrement positif afin d'éviter des neurones \"morts\".\n",
    "* On ajoute un peut de bruit pour briser symétrie (\"symmetry breaking\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "x3 = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_3 = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Première couche\n",
    "\n",
    "C'est une convolution de _stride_ 1 et _zero padded_, donc la sortie a la même taille que l'entrée. On suit la convolution du _max pooling_.\n",
    "\n",
    "La convolution calculera 32 critères pour chaque carré (_patch_) de 5$\\times$ 5.  Le tenseur de poids est de dimension $5\\times 5\\times 1\\times 32$.  Les dimensions sont (2) _patch size_, (1) nombres de chenals d'entrée, et (1) le nombre de chenals de sortie.\n",
    "\n",
    "Afin de l'appliquer, nous reformerons notre image ($28\\times 28$) à un tenseur 4-D.  Le quatrième dimension est le nombre de couleurs dans l'image.\n",
    "\n",
    "Finalement, nous passons par la convolution de l'image avec le poid, on ajoute le biais, passe par le ReLU et puis le max pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x3, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deuxième couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troisième couche\n",
    "\n",
    "*Densely connected layer.*  L'image est réduite à $7\\times 7$.  Nous ajoutons une couche entièrement connectée avec 1024 neurones afin de traiter l'image entière.\n",
    "\n",
    "Nous reformons le tenseur de la couche précédente (\"pooling layer\") dans une collection de vecteur, on multiplie pare la matrice de poids, ajoute un biais, et passe par un ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Afin de réduire le surapprentissage, nous ajoutons un dropout avant la sortie finale.  Le placeholder représente la probabilité qu'un neuronne est gardé pendant la phase de dropout.  Ainsi on peut utiliser le dropout en apprentissage mais pas en classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage et évaluation\n",
    "\n",
    "Ici c'est presque la même chose qu'avec softmax.  Les différences principales :\n",
    "* On remplace la descente du gradient avec ADAM.\n",
    "* On ajoute `keep_prob` au `feed_dict`.\n",
    "* On ajoute un message de logging toutes les 100 itéreation.\n",
    "\n",
    "**Attention : le code suivant fait 20K itérations et risque de prendre un moment (approx. 30 minutes) !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess3 = tf.Session()\n",
    "# num_iterations = 20 * 1000\n",
    "num_iterations = 800\n",
    "\n",
    "cross_entropy3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        y_conv, y_3))\n",
    "train_step3 = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy3)\n",
    "correct_prediction3 = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_3,1))\n",
    "accuracy3 = tf.reduce_mean(tf.cast(correct_prediction3, tf.float32))\n",
    "sess3.run(tf.initialize_all_variables())\n",
    "for i in range(num_iterations):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy3.eval(feed_dict={\n",
    "            x3: batch[0], y_3: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step3.run(feed_dict={x3: batch[0], y_3: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "            x3: mnist.test.images, \n",
    "            y_3: mnist.test.labels, \n",
    "            keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
